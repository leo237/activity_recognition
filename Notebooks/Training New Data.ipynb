{
 "metadata": {
  "name": "Training New Data"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Training begins here. We first import all the required import files. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import tensorflow as tf\nimport numpy as np\nimport pickle\nimport math\nimport random",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "We define certain utility functions below.\n\nFirst is to convert a number into one-hot encoding. Since we have 7 classes, it will turn an integer into a one-hot-encoding list with 7 values."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def oneHotEncoding(n):\n\tres = []\n\tfor each in n:\n\t\ts = [0 for i in xrange(7)]\n\t\ts[int(each)-1] = 1\n\t\tres.append(s)\n\treturn np.asarray(res)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Next, we define a function that will return the next batch of data that is used while training. We use mini-batch to train, instead of the entire data to calculate gradients and minimize loss function. This has proven to be slightly less accurate, but much faster to reach the minimum. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def nextBatch(data,datasize):\n\tresult = set()\n\ttotalDataSize = data[0].shape[0]\n\tfor x in range (0, datasize):\n\t    num = random.randint(0, totalDataSize-1)\n\t    while num in result:\n\t        num = random.randint(0, totalDataSize-1)\n\t    result.add(num)\n\tx = []\n\ty_ = []\n\tfor each in result:\n\t\tx.append(data[0][each,:])\n\t\ty_.append(data[1][each,:])\n\treturn np.asarray(x), np.asarray(y_)\n",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Here, a function is defined to initialize all the tensors. Here I'm using Xavier's initialization strategy as it has proven to give very good results. There's also provision to initialize using zeros and uniform data. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def init_weights(shape, namedAs, init_method='xavier', xavier_params = (None, None)):\n    if init_method == 'zeros':\n        return tf.Variable(tf.zeros(shape, dtype=tf.float32), name=namedAs)\n    elif init_method == 'uniform':\n        return tf.Variable(tf.random_normal(shape, stddev=0.01, dtype=tf.float32), name=namedAs)\n    else: #xavier\n        (fan_in, fan_out) = xavier_params\n        low = -4*np.sqrt(1.0/(fan_in + fan_out)) # {sigmoid:4, nn.relu:1} \n        high = 4*np.sqrt(1.0/(fan_in + fan_out))\n        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32), name=namedAs)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Now, the pickled files are loaded. Since we had 4 sets, we choose any three to train, and the fourth set to validate. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "train = [1,2,3]\nvalidate = 4\n\ndata = []\nfor i in train:\n\tfileName = '/Users/Leo/Desktop/clean/finalData/crossValidationPart'+str(i) +'.pickle'\n\tprint \"filename \", fileName\n\tpickledFile = open(fileName,'r')\n\ttempData = pickle.load(pickledFile)\n\ttempData = tempData.astype(np.float32)\n\tprint tempData.shape\n\ttempData = tempData.tolist()\n\tdata = data+tempData\n\tpickledFile.close()\n\ndata = np.asarray(data)\nprint data.shape\n\nvalidationSetFileName = fileName = '/Users/Leo/Desktop/clean/finalData/crossValidationPart'+str(validate) +'.pickle'\npickledFile = open(fileName,'r')\nvalidationData = pickle.load(pickledFile)\nvalidationData = validationData.astype(np.float32)\n\ninput_data = data[:,1:4]\nlabel_data = oneHotEncoding(data[:,4]) \ntrain_size = input_data.shape[0]\nacc_data = []\nacc_data.append(input_data)\nacc_data.append(label_data)\n\nvalidation_test_data = validationData[:,1:4]\nvalidation_label_data = oneHotEncoding(validationData[:,4])\n",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Now the input and input_label variables are declared."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "input_size = 3\noutput_size = 7 #Number of classes\n\n\nx = tf.placeholder(tf.float32, [None, input_size], 'x')\ny_ = tf.placeholder(tf.int32, [None, output_size], 'y_')",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Now our neural network is defined. \n\nIn the first hidden layer, we have 4 units. So the matrix formed will 3x4 because there are three input variables viz. acceleration in the direction of x,y and z. \n\nThe weights are initialized using xavier initialization strategy. The biases are initialized to zero.\n\nThe activation function that is being used is tanh. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "hidden1_units = 4\nweights1 = init_weights(\n        [input_size, hidden1_units], 'weights1',\n        'xavier',\n        xavier_params=(input_size, hidden1_units))\n\nbiases1 = init_weights([1,hidden1_units], 'biases1','zeros')\nhidden1 = tf.tanh(tf.matmul(x, weights1) + biases1)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Similary, we define hidden layers 2 and 3. \nHidden layer 2 has a dimension of 4x5\nHidden layer 3 has a dimension of 5x6\n\nBoth use tanh activation functions. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#Hidden Layer 2\nhidden2_units = 5\nweights2 = init_weights(\n        [hidden1_units, hidden2_units], 'weights2',\n        'xavier',\n        xavier_params=(hidden1_units, hidden2_units))\n\nbiases2 = init_weights([1,hidden2_units], 'biases2','zeros')\n\nhidden2 = tf.tanh(tf.matmul(hidden1, weights2))\n\n################################################################################\n#Hidden Layer 3\nhidden3_units = 6\nweights3 = init_weights(\n        [hidden2_units, hidden3_units], 'weights3',\n        'xavier',\n        xavier_params=(hidden2_units, hidden3_units))\n\nbiases3 = init_weights([1,hidden3_units], 'biases3','zeros')\n\nhidden3 = tf.tanh(tf.matmul(hidden2, weights3) + biases3)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Now we define the output layer. It is of dimension 6x7 as the output has to be classified to 7 labels. \n\nWeights are being initialized using xavier initialization strategy and biases to zero. \n\nWe do NOT use an activation funciton to the results of this layer. We perform just a linear operation here. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#Output Layer. Not tanh here. Linear operation. \nweights4 = init_weights(\n        [hidden3_units, output_size], 'weights4',\n        'xavier',\n        xavier_params=(hidden3_units, output_size))\nbiases4 = init_weights([1,output_size], 'biases4','zeros')\n\nlogits = tf.matmul(hidden3, weights4) + biases4",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "The loss function is defined here. We pass the logits that we get from training, the labels with which the logits are compared with and a regularization term that is added to the loss, to prevent over fitting. The loss function chosen is cross-entropy loss between logits and labels. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def loss(logits, labels,regularizers):\n\tlabels = tf.to_float(labels)\n\tcross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits,labels,name='xentropy')\n\tloss = tf.reduce_mean(cross_entropy,name='xentropy_mean')\n\tloss += 5e-4 * regularizers\n\treturn loss",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Now we define the learning rate. This is the most important step here. We tweak with this hyper parameter to get the best results"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "global_step = tf.Variable(0, trainable=False)\nlearning_rate = 3e-2",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Now we choose the kind of optimization technique that we intend to adopt. After testing simple Stochastic Gradient Descent, Adam Optimizer and RMSProp, I found better results with RMSProp. \n\nWe also use L2 loss for regularization. \n\nWe then initialize all our variables. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "optimizer = tf.train.RMSPropOptimizer(learning_rate,decay=0.99, momentum=0.5, epsilon=1e-10)\n\nregularizers = (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) +\n                  tf.nn.l2_loss(weights3) + tf.nn.l2_loss(weights4))\n\ncomputedLoss = loss(logits,y_,regularizers)\n\ntrain = optimizer.minimize(computedLoss,global_step=global_step)\n\ninit = tf.initialize_all_variables()",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Then we define a saver variable that is going to help us save our model from time to time. Then we launch our graph"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "saver = tf.train.Saver()\n\nsess = tf.Session()\nsess.run(init)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Now we begin training our data. \n\nWe get a set of x and y_ using our nextBatch function that was previously defined. \n\nThe loss is calculated after every step. When the step is a multiple of 1000, we print the loss. This helps us get an idea about how training is proceeding. \n\nWhen the step is a mutliple of 5000/10000, we first save the model that has been built till now, and then test the accuracy achieved by this model. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "for step in xrange(100001):\n\tnext_x, next_y_ = nextBatch(acc_data,4000)\n\t_, losss = sess.run([train, computedLoss], feed_dict={x: next_x, y_: next_y_})\n\tif step%1000 == 0:\n\t\tprint step ,\n\t\tprint \"  Loss: \",losss\n\tif step%5000 == 0:\n\t\tsave_path = saver.save(sess, \"/Users/Leo/Desktop/clean/models/newCrossValidation1/attempt1/model_\"+str(step)+\"_\"+str(losss)+\".ckpt\")\n \t\tprint(\"Model saved in file: %s\" % save_path)\n# \tif step%10000 == 0:\t\n \t\tcorrect_prediction = tf.equal(tf.argmax(logits,1)+1, tf.argmax(y_,1)+1)\n\t\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\t\tprint \"ACCURACY : \", \n\t\tprint(sess.run(accuracy, feed_dict={x: validation_test_data, y_: validation_label_data}))",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}