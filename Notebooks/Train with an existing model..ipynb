{
 "metadata": {
  "name": "Train with an existing model."
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Everything is the same as the training for a new model, till the step where we initialize variables. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import tensorflow as tf\nimport numpy as np\nimport pickle\nimport math\nimport random\n\n\n################################################################################\n#Load Data from picked files\ntrain = [2,3,4]\nvalidate = 1\n\ndata = []\nfor i in train:\n\tfileName = '/Users/Leo/Desktop/clean/finalData/crossValidationPart'+str(i) +'.pickle'\n\tprint \"filename \", fileName\n\tpickledFile = open(fileName,'r')\n\ttempData = pickle.load(pickledFile)\n\ttempData = tempData.astype(np.float32)\n\tprint tempData.shape\n\ttempData = tempData.tolist()\n\tdata = data+tempData\n\tpickledFile.close()\n\ndata = np.asarray(data)\nprint data.shape\n\nvalidationSetFileName = fileName = '/Users/Leo/Desktop/clean/finalData/crossValidationPart'+str(validate) +'.pickle'\npickledFile = open(fileName,'r')\nvalidationData = pickle.load(pickledFile)\nvalidationData = validationData.astype(np.float32)\n\n################################################################################\n# Utility Functions\n\ndef oneHotEncoding(n):\n\tres = []\n\tfor each in n:\n\t\ts = [0 for i in xrange(7)]\n\t\ts[int(each)-1] = 1\n\t\tres.append(s)\n\treturn np.asarray(res)\n\ndef nextBatch(data,datasize):\n\tresult = set()\n\ttotalDataSize = data[0].shape[0]\n\tfor x in range (0, datasize):\n\t    num = random.randint(0, totalDataSize-1)\n\t    while num in result:\n\t        num = random.randint(0, totalDataSize-1)\n\t    result.add(num)\n\tx = []\n\ty_ = []\n\tfor each in result:\n\t\tx.append(data[0][each,:])\n\t\ty_.append(data[1][each,:])\n\treturn np.asarray(x), np.asarray(y_)\n\ndef init_weights(shape, namedAs, init_method='xavier', xavier_params = (None, None)):\n    if init_method == 'zeros':\n        return tf.Variable(tf.zeros(shape, dtype=tf.float32), name=namedAs)\n    elif init_method == 'uniform':\n        return tf.Variable(tf.random_normal(shape, stddev=0.01, dtype=tf.float32), name=namedAs)\n    else: #xavier\n        (fan_in, fan_out) = xavier_params\n        low = -4*np.sqrt(1.0/(fan_in + fan_out)) # {sigmoid:4, nn.relu:1} \n        high = 4*np.sqrt(1.0/(fan_in + fan_out))\n        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32), name=namedAs)\n\n################################################################################\n# Define Input and Label data variables\n\ninput_data = data[:,1:4]\nlabel_data = oneHotEncoding(data[:,4]) \ntrain_size = input_data.shape[0]\nacc_data = []\nacc_data.append(input_data)\nacc_data.append(label_data)\n\nvalidation_test_data = validationData[:,1:4]\nvalidation_label_data = oneHotEncoding(validationData[:,4])\n\n################################################################################\n#Define input and output tensor placeholders\n\ninput_size = 3\noutput_size = 7 #Number of classes\n\n\nx = tf.placeholder(tf.float32, [None, input_size], 'x')\ny_ = tf.placeholder(tf.int32, [None, output_size], 'y_')\n\n################################################################################\n\n#Hidden Layer 1\nhidden1_units = 4\nweights1 = init_weights(\n        [input_size, hidden1_units], 'weights1',\n        'xavier',\n        xavier_params=(input_size, hidden1_units))\n\nbiases1 = init_weights([1,hidden1_units], 'biases1','zeros')\nhidden1 = tf.nn.relu(tf.matmul(x, weights1) + biases1)\n\n################################################################################\n#Hidden Layer 2\nhidden2_units = 5\nweights2 = init_weights(\n        [hidden1_units, hidden2_units], 'weights2',\n        'xavier',\n        xavier_params=(hidden1_units, hidden2_units))\n\nbiases2 = init_weights([1,hidden2_units], 'biases2','zeros')\n\nhidden2 = tf.nn.relu(tf.matmul(hidden1, weights2))\n\n################################################################################\n#Hidden Layer 3\nhidden3_units = 6\nweights3 = init_weights(\n        [hidden2_units, hidden3_units], 'weights3',\n        'xavier',\n        xavier_params=(hidden2_units, hidden3_units))\n\nbiases3 = init_weights([1,hidden3_units], 'biases3','zeros')\n\nhidden3 = tf.nn.relu(tf.matmul(hidden2, weights3) + biases3)\n\n################################################################################\n#Output Layer. Not nn.relu here. Linear operation. \nweights4 = init_weights(\n        [hidden3_units, output_size], 'weights4',\n        'xavier',\n        xavier_params=(hidden3_units, output_size))\nbiases4 = init_weights([1,output_size], 'biases4','zeros')\n\nlogits = tf.matmul(hidden3, weights4) + biases4\n\n################################################################################\n# Define Loss here\ndef loss(logits, labels,regularizers):\n\tlabels = tf.to_float(labels)\n\tcross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits,labels,name='xentropy')\n\tloss = tf.reduce_mean(cross_entropy,name='xentropy_mean')\n\tloss += 5e-4 * regularizers\n\treturn loss\n\n################################################################################\n# Define for training\nglobal_step = tf.Variable(0, trainable=False)\nlearning_rate = 1e-2\n\n# Learning rate decay. Not required for Adam Optimizer\n# learning_rate = tf.train.exponential_decay(\n#       learning_rate,        \t# Base learning rate.\n#       global_step * 500,\t\t# Current index into the dataset.\n#       train_size,          \t# Decay step.\n#       0.95,                \t# Decay rate.\n#       staircase=True)\t\t\t\n\noptimizer = tf.train.AdamOptimizer(learning_rate)\n\nregularizers = (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) +\n                  tf.nn.l2_loss(weights3) + tf.nn.l2_loss(weights4))\n\ncomputedLoss = loss(logits,y_,regularizers)\n\ntrain = optimizer.minimize(computedLoss,global_step=global_step)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Then we define the saver variable. And begin our session."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "saver = tf.train.Saver()\nsess = tf.Session()",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "Now instead of initializing with one of the initialization strategies, we initialize from one of the pre saved models."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "saver.restore(sess,\"/Users/Leo/Desktop/clean/models/crossValidation1/attempt_1/model_195000_1.16012.ckpt\")",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "We continue training as before."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "for step in xrange(100001):\n\tnext_x, next_y_ = nextBatch(acc_data,4000)\n\t_, losss = sess.run([train, computedLoss], feed_dict={x: next_x, y_: next_y_})\n\tif step%1000 == 0:\n\t\tprint step ,\n\t\tprint \"  Loss: \",losss\n\tif step%5000 == 0:\n\t\tsave_path = saver.save(sess, \"/Users/Leo/Desktop/clean/models/newCrossValidation1/attempt2/model_\"+str(step)+\"_\"+str(losss)+\".ckpt\")\n \t\tprint(\"Model saved in file: %s\" % save_path)\n# \tif step%10000 == 0:\t\n \t\tcorrect_prediction = tf.equal(tf.argmax(logits,1)+1, tf.argmax(y_,1)+1)\n\t\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\t\tprint \"ACCURACY : \", \n\t\tprint(sess.run(accuracy, feed_dict={x: validation_test_data, y_: validation_label_data}))",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}